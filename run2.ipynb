{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "import time\n",
    "import torch\n",
    "from datacreate import MyPCQM4MDataset\n",
    "from GEvaluator import Evaluator\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from Mid import GINGraphPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class MyNamespace(argparse.Namespace):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.batch_size = 5\n",
    "        self.device=0\n",
    "        self.drop_ratio=0.1\n",
    "        self.early_stop=20\n",
    "        self.early_stop_open = True\n",
    "        self.emb_dim=256\n",
    "        self.epochs=200\n",
    "        self.graph_pooling='sum'\n",
    "        self.num_layers=3\n",
    "        self.n_head=3\n",
    "        self.num_workers=5\n",
    "        self.num_tasks=1\n",
    "        self.save_test=True\n",
    "        self.task_name='GINGraph-test'\n",
    "        self.weight_decay=0.5e-05\n",
    "        self.learning_rate=0.00001\n",
    "        self.root='./dataset'\n",
    "        self.dataset_use_pt=True\n",
    "        self.dataset_pt = './PTs/'\n",
    "        self.dataset_split=[0.5,0.1,0.1]\n",
    "        self.begin=0\n",
    "        self.dataset_length=50000\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "参数输入"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    if  args.dataset_use_pt:\n",
    "        if os.path.isdir(args.dataset_pt):\n",
    "            print('data loading in dir:',args.dataset_pt)\n",
    "            dataset=[]\n",
    "            for filename in tqdm(os.listdir(args.dataset_pt)):\n",
    "                dataset.extend(torch.load(os.path.join(args.dataset_pt,filename)))\n",
    "                # print(len(dataset))\n",
    "        else:\n",
    "            print('data loading in .pt:', args.dataset_pt)\n",
    "            dataset=torch.load(args.dataset_pt)\n",
    "    else:\n",
    "        print('MyPCQM4MDataset: data loading from',args.begin,'to',args.begin+args.dataset_length)\n",
    "        dataset = MyPCQM4MDataset(args.root,save=False,begin=args.begin,length=args.dataset_length)\n",
    "\n",
    "    length=len(dataset)\n",
    "    train_idx=round(length*args.dataset_split[0])\n",
    "    valid_idx=round(length*(args.dataset_split[0]+args.dataset_split[1]))\n",
    "\n",
    "    train_data=dataset[0:train_idx]\n",
    "    valid_data=dataset[train_idx:valid_idx]\n",
    "    test_data=dataset[valid_idx:-1]\n",
    "\n",
    "    print('train data:',len(train_data),'valid data:',len(valid_data))\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "    test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "    return train_loader,valid_loader,test_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据载入"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def train(model, device, loader, optimizer, criterion_fn,epoch,epochs):\n",
    "    print('on training:')\n",
    "    model.train()\n",
    "    loss_accum = 0\n",
    "    maxP = 0\n",
    "    minN = 0\n",
    "    avgP = 0\n",
    "    avgN = 0\n",
    "\n",
    "    pbar=tqdm(total = len(loader), desc=f'Epoch {epoch}/{epochs}', unit='it')\n",
    "    for step, batch in enumerate(loader):  # 枚举所有批次的数据\n",
    "        # print(step)\n",
    "        # print(type(batch))\n",
    "        batch = batch.to(device)  # 将数据移动到指定的设备\n",
    "        # print(batch.x)\n",
    "        pred = model(batch).view(-1, )  # 前向传播，计算预测值\n",
    "        try:\n",
    "            assert not torch.any(torch.isnan(pred))\n",
    "        except:\n",
    "            print(batch.new_pos,batch.x,batch.y)\n",
    "            break\n",
    "        # print(pred.shape)\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        deltayy=pred-batch.y.view(pred.shape)\n",
    "        try:\n",
    "            if maxP<torch.max(deltayy[deltayy>0]):\n",
    "                maxP=max(deltayy[deltayy>0])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if minN>torch.min(deltayy[deltayy<0]):\n",
    "                minN=min(deltayy[deltayy<0])\n",
    "        except:\n",
    "            pass\n",
    "        avgP+=torch.mean(deltayy[deltayy>0])\n",
    "        avgN+=torch.mean(deltayy[deltayy<0])\n",
    "        # print(deltayy)\n",
    "        # print(pred.shape,batch.y.shape)\n",
    "        loss = criterion_fn(pred, batch.y.view(pred.shape))  # 计算损失\n",
    "        assert not torch.any(torch.isnan(loss))\n",
    "\n",
    "        pbar.set_postfix({'loss' : '{0:1.5f}'.format(loss)}) #在进度条后显示当前batch的损失\n",
    "        pbar.update(1) #更当前进度，1表示完成了一个batch的训练\n",
    "\n",
    "        loss.backward()  # 反向传播，计算梯度\n",
    "        optimizer.step()  # 更新模型参数\n",
    "        loss_accum += loss.detach().cpu().item()  # 累加损失值\n",
    "\n",
    "    return loss_accum / (step + 1),maxP,minN,avgP / (step + 1),avgN / (step + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def eval(model, device, loader, evaluator):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算，加速模型运算\n",
    "        for _, batch in enumerate(loader):  # 枚举所有批次的数据\n",
    "            # print('test',type(batch))\n",
    "            batch = batch.to(device)  # 将数据移动到指定的设备\n",
    "            pred = model(batch).view(-1, )  # 前向传播，计算预测值\n",
    "            y_true.append(batch.y.view(pred.shape).detach().cpu())  # 将真实值添加到列表中\n",
    "            y_pred.append(pred.detach().cpu())  # 将预测值添加到列表中\n",
    "\n",
    "    y_true = torch.cat(y_true, dim=0)  # 拼接真实值列表成一个张量\n",
    "    y_pred = torch.cat(y_pred, dim=0)  # 拼接预测值列表成一个张量\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}  # 构造输入字典\n",
    "    return evaluator.eval(input_dict)[\"mae\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def test(model, device, loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算，加速模型运算\n",
    "        for _, batch in enumerate(loader):  # 枚举所有批次的数据\n",
    "            batch = batch.to(device)  # 将数据移动到指定的设备\n",
    "            pred = model(batch).view(-1, )  # 前向传播，计算预测值\n",
    "            y_pred.append(pred.detach().cpu())  # 将预测值添加到列表中\n",
    "\n",
    "    y_pred = torch.cat(y_pred, dim=0)  # 拼接预测值列表成一个张量\n",
    "    return y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def prepartion(args):\n",
    "    save_dir = os.path.join('saves', args.task_name+'_')\n",
    "    if os.path.exists(save_dir):\n",
    "        for idx in range(1000):\n",
    "            if not os.path.exists(save_dir + '=' + str(idx)):\n",
    "                save_dir = save_dir + '=' + str(idx)\n",
    "                break\n",
    "\n",
    "    args.save_dir = save_dir\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    args.device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    args.output_file = open(os.path.join(args.save_dir, 'output'), 'a')\n",
    "    print(args, file=args.output_file, flush=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    prepartion(args)\n",
    "    nn_params = {\n",
    "        'num_layers': args.num_layers,\n",
    "        'emb_dim': args.emb_dim,\n",
    "        'n_head':args.n_head,\n",
    "        'drop_ratio': args.drop_ratio,\n",
    "        'graph_pooling': args.graph_pooling,\n",
    "        'num_tasks':args.num_tasks,\n",
    "        'batchsize':args.batch_size\n",
    "\n",
    "    }\n",
    "\n",
    "    # automatic dataloading and splitting\n",
    "    train_loader,valid_loader,test_loader=load_data(args)\n",
    "\n",
    "    # automatic evaluator. takes dataset name as input\n",
    "    evaluator = Evaluator()\n",
    "    criterion_fn = torch.nn.MSELoss()\n",
    "\n",
    "    device = args.device\n",
    "\n",
    "    model = GINGraphPooling(**nn_params).to(device)\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'#Params: {num_params}', file=args.output_file, flush=True)\n",
    "    print(model, file=args.output_file, flush=True)\n",
    "\n",
    "    optimizer =  torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.25)\n",
    "\n",
    "\n",
    "    writer = SummaryWriter(log_dir=args.save_dir)\n",
    "\n",
    "    not_improved = 0\n",
    "    best_valid_mae = 9999\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "\n",
    "        print('epoch:', epoch,time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) )\n",
    "\n",
    "        print(\"=====Epoch {}\".format(epoch), file=args.output_file, flush=True)\n",
    "        print('Training...', file=args.output_file, flush=True)\n",
    "        train_mae,maxP,minN,avgP,avgN = train(model, device, train_loader, optimizer, criterion_fn,epoch,args.epochs)\n",
    "        print(train_mae,maxP,minN,avgP,avgN)\n",
    "        print('Evaluating...', file=args.output_file, flush=True)\n",
    "        valid_mae = eval(model, device, valid_loader, evaluator)\n",
    "\n",
    "        print({'Train': train_mae, 'Validation': valid_mae}, file=args.output_file, flush=True)\n",
    "\n",
    "        writer.add_scalar('valid/mae', valid_mae, epoch)\n",
    "        writer.add_scalar('train/mae', train_mae, epoch)\n",
    "        writer.add_scalar('train/maxP', maxP, epoch)\n",
    "        writer.add_scalar('train/minN', minN, epoch)\n",
    "        writer.add_scalar('train/avgP', avgP, epoch)\n",
    "        writer.add_scalar('train/avgN', avgN, epoch)\n",
    "\n",
    "\n",
    "\n",
    "        if valid_mae < best_valid_mae:\n",
    "            best_valid_mae = valid_mae\n",
    "            if args.save_test:\n",
    "                print('Saving checkpoint...', file=args.output_file, flush=True)\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(), 'best_val_mae': best_valid_mae, 'num_params': num_params\n",
    "                }\n",
    "                torch.save(checkpoint, os.path.join(args.save_dir, 'checkpoint.pt'))\n",
    "                print('Predicting on test data...', file=args.output_file, flush=True)\n",
    "                y_pred = test(model, device, test_loader)\n",
    "                print('Saving test submission file...', file=args.output_file, flush=True)\n",
    "                evaluator.save_test_submission({'y_pred': y_pred}, args.save_dir)\n",
    "\n",
    "            not_improved = 0\n",
    "        else:\n",
    "            not_improved += 1\n",
    "            if not_improved == args.early_stop:\n",
    "                print(f\"Have not improved for {not_improved} epoches.\", file=args.output_file, flush=True)\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f'Best validation MAE so far: {best_valid_mae}', file=args.output_file, flush=True)\n",
    "\n",
    "    # writer.add_graph(model,train_loader)\n",
    "    writer.close()\n",
    "    args.output_file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": data loading in dir: ./PTs/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/2000 [02:47<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49985 49985 59982 9997 99970\n",
      "epoch: 1 2023-07-17 16:29:32\n",
      "All samples: 49985\n",
      "on training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/9997 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # args=p_args()\n",
    "    args=MyNamespace()\n",
    "    main(args)\n",
    "    print('finish')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
